"""Main FastAPI application."""
import os
import logging
import json
from pathlib import Path
from typing import List, Optional

from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import uvicorn
import asyncio

# Load environment variables from .env file (before importing config)
env_path = Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path)

try:
    from . import config
    from . import task_queue
    from . import storage
    from . import worker
    from . import models
except ImportError:
    # Allow running as script
    import config
    import task_queue
    import storage
    import worker
    import models

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown events."""
    # Startup
    logger.info("Starting MinerU GPU Server on {}:{}".format(
        config.HOST, config.PORT))
    logger.info("MinerU Backend: {}".format(config.MINERU_BACKEND))
    logger.info("GPU Available: {}".format(
        os.getenv('CUDA_VISIBLE_DEVICES', 'not_set')))
    logger.info("Max Concurrent Workers: {}".format(
        config.MAX_CONCURRENT_WORKERS))

    # Initialize Supabase
    if config.SUPABASE_URL and config.SUPABASE_SECRET_KEY:
        storage.init_supabase(config.SUPABASE_URL, config.SUPABASE_SECRET_KEY)

    # Initialize Redis
    task_queue.init_redis(config.REDIS_URL)
    logger.info("Redis: {}".format("Connected" if task_queue.is_redis_configured(
    ) else "Not available (using in-memory)"))

    # Initialize worker
    worker.init_worker()

    # Start multiple worker loops for parallel processing
    for i in range(config.MAX_CONCURRENT_WORKERS):
        asyncio.create_task(worker.worker_loop())
        logger.info(
            f"Worker loop {i+1}/{config.MAX_CONCURRENT_WORKERS} started")

    yield

    # Shutdown (if needed)
    logger.info("Shutting down...")


app = FastAPI(title="MinerU GPU Server", lifespan=lifespan)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
async def root():
    """Health check endpoint."""
    try:
        import torch
        gpu_available = torch.cuda.is_available() if hasattr(torch, 'cuda') else False
        gpu_count = torch.cuda.device_count() if gpu_available else 0
    except:
        gpu_available = False
        gpu_count = 0

    return {
        "status": "ok",
        "service": "MinerU GPU Server",
        "gpu_available": gpu_available,
        "gpu_count": gpu_count,
        "mineru_backend": config.MINERU_BACKEND,
        "queue_length": task_queue.get_queue_length(),
        "processing": task_queue.get_processing_count(),
        "max_workers": config.MAX_CONCURRENT_WORKERS
    }


@app.get("/health")
async def health():
    """Detailed health check."""
    try:
        import torch
        gpu_available = torch.cuda.is_available() if hasattr(torch, 'cuda') else False
        gpu_count = torch.cuda.device_count() if gpu_available else 0
        gpu_name = torch.cuda.get_device_name(0) if gpu_available else None
    except:
        gpu_available = False
        gpu_count = 0
        gpu_name = None

    return {
        "status": "healthy",
        "gpu_available": gpu_available,
        "gpu_count": gpu_count,
        "gpu_name": gpu_name,
        "mineru_backend": config.MINERU_BACKEND,
        "supabase_configured": config.SUPABASE_URL is not None,
        "redis_configured": task_queue.is_redis_configured(),
        "cuda_visible_devices": os.getenv("CUDA_VISIBLE_DEVICES", "not_set"),
        "queue_length": task_queue.get_queue_length(),
        "processing": task_queue.get_processing_count(),
        "max_workers": config.MAX_CONCURRENT_WORKERS
    }


@app.get("/status/{task_id}")
async def get_status(task_id: str):
    """Get task status by ID."""
    status = task_queue.get_task_status(task_id)
    if not status:
        raise HTTPException(status_code=404, detail="Task not found")
    return status


@app.post("/process", response_model=models.BatchProcessResponse)
async def process_endpoint(
    request: Request,
    files: List[UploadFile] = File(...),
    user_id: str = Form(...),
    document_ids: str = Form(
        ..., description="JSON array of document IDs (one per file), generated by Railway"),
    metadatas: Optional[str] = Form(
        None, description="JSON array of metadata objects, one per file"),
    upload_to_storage: bool = Form(True),
    index: bool = Form(True)
):
    """
    Process one or more PDF files with MinerU GPU.
    Returns immediately with task IDs. Use /status/{task_id} to check progress.
    """
    # Validate API key if configured
    if config.API_KEY:
        api_key = request.headers.get("X-API-Key")
        if not api_key or api_key != config.API_KEY:
            raise HTTPException(
                status_code=401,
                detail="Invalid or missing API key. Provide X-API-Key header."
            )

    files_list = files if isinstance(files, list) else [files]

    if len(files_list) == 0:
        raise HTTPException(
            status_code=400, detail="At least one PDF file is required")

    # Parse document IDs (required - Railway generates these)
    try:
        document_ids_list = json.loads(document_ids)
        if not isinstance(document_ids_list, list):
            raise HTTPException(
                status_code=400, detail="document_ids must be a JSON array")
        if len(document_ids_list) != len(files_list):
            raise HTTPException(
                status_code=400, detail="document_ids array length must match files array length")
    except json.JSONDecodeError:
        raise HTTPException(
            status_code=400, detail="Invalid JSON in document_ids")

    # Parse metadatas
    metadatas_list = []
    if metadatas:
        try:
            metadatas_list = json.loads(metadatas)
            if not isinstance(metadatas_list, list):
                metadatas_list = []
        except:
            metadatas_list = []

    # Pad metadatas to match file count
    while len(metadatas_list) < len(files_list):
        metadatas_list.append(None)

    # Enqueue all files
    task_ids = []
    for i, file in enumerate(files_list):
        # Validate file
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            raise HTTPException(
                status_code=400, detail=f"File {file.filename} is not a PDF")

        # Use document ID from Railway (not generated here)
        document_id = document_ids_list[i]

        # Read file content
        file_content = await file.read()

        # Create task
        task_data = {
            "user_id": user_id,
            "document_id": document_id,
            "filename": file.filename,
            "file_data": file_content,
            "metadata": metadatas_list[i] if i < len(metadatas_list) else None,
            "upload_to_storage": upload_to_storage,
            "index": index
        }

        # Enqueue task
        task_id = task_queue.enqueue_task(task_data)
        task_ids.append(task_id)

    # Always return BatchProcessResponse format for consistency
    return models.BatchProcessResponse(
        total=len(files_list),
        tasks=[
            models.ProcessResponse(
                task_id=tid, status_url=f"/status/{tid}")
            for tid in task_ids
        ]
    )


if __name__ == "__main__":
    import os
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ImportError:
        pass

    logger.info(f"Starting MinerU GPU Server on {config.HOST}:{config.PORT}")
    logger.info(f"MinerU Backend: {config.MINERU_BACKEND}")
    logger.info(
        f"GPU Available: {os.getenv('CUDA_VISIBLE_DEVICES', 'not_set')}")
    logger.info(f"Max Concurrent Workers: {config.MAX_CONCURRENT_WORKERS}")
    logger.info(
        f"Redis: {'Connected' if task_queue.is_redis_configured() else 'Not available (using in-memory)'}")

    uvicorn.run(
        app,
        host=config.HOST,
        port=config.PORT,
        log_level="info"
    )
